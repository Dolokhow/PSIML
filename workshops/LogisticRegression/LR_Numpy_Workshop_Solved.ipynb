{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on PSIML 2018 workshop\n",
    "## Numpy workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define array\n",
    "a = np.array([1,2,3])\n",
    "\n",
    "# Some basic properties\n",
    "print(\"Array a: \", a)\n",
    "print(\"\\nShape of array a: \", a.shape)\n",
    "print(\"\\nData type of array a: \", a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrix\n",
    "b = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Some basic properties\n",
    "print(\"Matrix b: \\n\", b)\n",
    "print(\"\\nShape of matrix b: \", b.shape)\n",
    "print(\"\\nData type of matrix b: \", b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multidim arrays - tensor\n",
    "c = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\n",
    "\n",
    "# Some basic properties\n",
    "print(\"Tensor c: \\n\", c)\n",
    "print(\"\\nShape of tensor c: \", c.shape)\n",
    "print(\"\\nData type of tensor c: \", c.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All zeros\n",
    "print(\"All zeros: \\n\", np.zeros((2,2)))\n",
    "\n",
    "# All ones\n",
    "print(\"\\nAll ones: \\n\", np.ones((2,2)))\n",
    "\n",
    "# All same value\n",
    "print(\"\\nAll same value: \\n\", np.full((2,2), 2))\n",
    "\n",
    "# All random\n",
    "# Setting a random seed is important for reproducibility of the code.\n",
    "# It is good practice to use it in ML before moving to actual training as it makes debuging a lot easier.\n",
    "np.random.seed(5)\n",
    "print(\"\\nAll random: \\n\", np.random.random((2,2)))\n",
    "\n",
    "# Identity matrix\n",
    "print(\"\\nIdentity matrix: \\n\", np.eye(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array indexing\n",
    "\n",
    "Indexing goes from 0 for the first element. It is possible to use negative indexes (for example -1 for last element of array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Array a: \", a)\n",
    "print(\"First element of a: \", a[0])\n",
    "print(\"Last element of a: \", a[2])\n",
    "print(\"Last element of a: \", a[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing in matrix and tensor is the same and we can index any column, row etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tensor c: \\n\", c)\n",
    "print(\"\\nValue of c[0]: \\n\", c[0])\n",
    "print(\"\\nValue of c[-2]: \\n\", c[-2])\n",
    "print(\"\\nValue of c[0][1]: \", c[0][1])\n",
    "print(\"Value of c[0][0][0]: \", c[0][0][0])\n",
    "print(\"Value of c[0, 0, 0]: \", c[0, 0, 0])\n",
    "print(\"\\nValue of c[0, :, 0:2]: \\n\", c[0, :, 0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2], [3, 4]], dtype=np.float64)\n",
    "y = np.array([[5, 6], [7, 8]], dtype=np.float64)\n",
    "\n",
    "print(\"Matrix x: \\n\", x)\n",
    "print(\"\\nMatrix y: \\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Addition:\\n\", x + y)\n",
    "print(\"Substruction:\\n\", y - x)\n",
    "print(\"Elementwise multiplication:\\n\", x * y)\n",
    "print(\"Multiplication:\\n\", np.matmul(x, y))\n",
    "print(\"Divion:\\n\", x / y)\n",
    "print(\"Square root:\\n\", np.sqrt(x))\n",
    "print(\"Exp:\\n\", np.exp(x))\n",
    "print(\"Dot product:\\n\", np.dot(x[1], y[0]))\n",
    "print(\"Transpose:\\n\", x.T)\n",
    "print(\"Inverse:\\n\", np.linalg.inv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "Broadcasting is one of the most important numpy features. The term broadcasting describes how numpy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = np.array([2.0, 2.0, 2.0])\n",
    "print(\"a * b, a as vector, b as vector:\", a * b)\n",
    "\n",
    "b = np.array([2])\n",
    "print(\"a * b, a as vector, b as scalar:\", a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3], [4,5,6]])\n",
    "b = np.array([2,4,6])\n",
    "\n",
    "print(\"a + b, a as matrix, b as vector:\\n\", a + b)\n",
    "print(\"a * b, a as matrix, b as vector:\\n\", a * b)\n",
    "print(\"Dot product of a and b:\\n\", np.dot(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important ML functions:\n",
    "### Sigmoid function:\n",
    "\n",
    "\\begin{equation*}\n",
    "S(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation*}\n",
    "\n",
    "You can find more at *https://en.wikipedia.org/wiki/Sigmoid_function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # [TODO] Implement sigmoid computation\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sigmoid of \\\"0\\\":\", sigmoid(0))\n",
    "print(\"Expected value: 0.5\")\n",
    "testArray = np.array([1,5])\n",
    "print(\"Sigmoid of [1,5]:\", sigmoid(testArray))\n",
    "print(\"Expected value: [0.73105858 0.99330715]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu function:\n",
    "\n",
    "\\begin{equation*}\n",
    "f(x)  = \\begin{cases}\n",
    "    x & \\mbox{if } x > 0 \\\\\n",
    "    0 & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "You can find more at *https://en.wikipedia.org/wiki/Rectifier_(neural_networks)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # [TODO] Implement ReLu funcion\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Relu of \\\"-5\\\":\", relu(-5))\n",
    "print(\"Expected value: 0\")\n",
    "\n",
    "print(\"Relu of \\\"5\\\":\", relu(5))\n",
    "print(\"Expected value: 5\")\n",
    "\n",
    "testArray = np.array([3,0,-1,2,5,-2])\n",
    "print(\"Relu of [3,0,-1,2,5,-2]:\", relu(testArray))\n",
    "print(\"Expected value: [3 0 0 2 5 0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10., 10., 0.2)\n",
    "rel = relu(x)\n",
    "plt.plot(x,rel)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(y=i\\mid \\mathbf{x}) = \\frac{e^{\\mathbf{x}_i}}{\\sum_{k=1}^K e^{\\mathbf{x}_k}}\n",
    "\\end{equation*}\n",
    "\n",
    "You can find more at *https://en.wikipedia.org/wiki/Softmax_function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # [TODO] Implement softmax function\n",
    "    return np.exp(x-max(x)) / np.sum(np.exp(x-max(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testArray = np.array([-1,0.1899, 0.4449, 0.98990])\n",
    "print(\"Softmax of [-1,0.1899,0.4449,0.98990]: \", softmax(testArray))\n",
    "print(\"Expected value: [0.06311943 0.20745794 0.26771651 0.46170613]\")\n",
    "testArray2 = np.array([1000, 1000, 1000])\n",
    "print(\"Softmax of [1000, 1000, 1000]: \", softmax(testArray2))\n",
    "print(\"Expected value: [0.33333333 0.33333333 0.33333333]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal equations\n",
    "\n",
    "Standard variant\n",
    "\\begin{equation}\n",
    "w = (X^TX)^{-1}X^Ty\n",
    "\\end{equation}\n",
    "\n",
    "Regularized variant\n",
    "\\begin{equation}\n",
    "w = (X^TX+\\lambda I)^{-1}X^Ty\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fixing random seed\n",
    "np.random.seed(5)\n",
    "\n",
    "# Generating synthetic data from exponential distribution with some noise\n",
    "sampleSize = 20\n",
    "x = sorted(np.random.uniform(0, 4, sampleSize))\n",
    "y = np.exp(x) + np.random.normal(0, 0.01, sampleSize)\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.show()\n",
    "\n",
    "# Adding column of ones for implicit treatment of bias term\n",
    "x = np.concatenate((np.ones((sampleSize,1)), np.reshape(x, (sampleSize, 1))), 1)\n",
    "\n",
    "# TODO estimate w via normal equations\n",
    "w = np.matmul(np.matmul(np.linalg.inv(np.matmul(x.T, x)), x.T), y)\n",
    "\n",
    "# TODO compute regression values using w\n",
    "z = np.matmul(x, w)\n",
    "\n",
    "xo = x[:, 1];\n",
    "plt.plot(xo, y, 'bo');\n",
    "plt.plot(xo, z)\n",
    "plt.show()\n",
    "\n",
    "# Generating powers of x up to some degree\n",
    "degree = 12\n",
    "for i in range(1, degree):\n",
    "    t = np.reshape(x[:, -1] * xo, (sampleSize, 1))\n",
    "    x = np.concatenate((x, t), 1)\n",
    "    \n",
    "# TODO play with regularization parameter to tune ridge regression\n",
    "lmbd = 0.000001\n",
    "\n",
    "# TODO estimate w using regularized normal equations\n",
    "w = np.matmul(np.matmul(np.linalg.inv(lmbd * np.eye(x.shape[1]) + np.matmul(x.T, x)), x.T), y)\n",
    "\n",
    "# TODO compute regression values using w\n",
    "u = np.matmul(x, w)\n",
    "\n",
    "plt.plot(xo, y, 'bo');\n",
    "plt.plot(xo, u)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem and data taken from *https://www.kaggle.com/c/titanic*\n",
    "\n",
    "## Goal \n",
    "\n",
    "Based on the provided information about person predict if person survived Titanic crash or not.\n",
    "\n",
    "## Feature explanation\n",
    "\n",
    "| Variable | Definition | Key |\n",
    "| ------------- | ------------- | ------------- |\n",
    "| survival | Survival | 0 = No, 1 = Yes |\n",
    "| pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "| sex | Sex | |\n",
    "| Age | Age in years | |\n",
    "| sibsp | # of siblings / spouses aboard the Titanic | |\n",
    "| parch | # of parents / children aboard the Titanic | |\n",
    "| ticket | Ticket number | |\n",
    "| fare | Passenger fare | |\n",
    "| cabin | Cabin number | |\n",
    "| embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "### Variable Notes\n",
    "\n",
    "**pclass**: Ticket class serving as a proxy for socio-economic status (SES)  \n",
    "1st = Upper  \n",
    "2nd = Middle  \n",
    "3rd = Lower  \n",
    "\n",
    "\n",
    "**age**: Age is fractional if less than 1. If the age is estimated, it is in the form of xx.5  \n",
    "\n",
    "**sibsp**: The dataset defines family relations in this way...  \n",
    "Sibling = brother, sister, stepbrother, stepsister  \n",
    "Spouse = husband, wife (mistresses and fiancés were ignored)  \n",
    "\n",
    "**parch**: The dataset defines family relations in this way...  \n",
    "Parent = mother, father  \n",
    "Child = daughter, son, stepdaughter, stepson  \n",
    "\n",
    "Some children travelled only with a nanny, therefore parch=0 for them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "path = r'Data\\train.csv'\n",
    "\n",
    "def readCSVasNumpy(path):\n",
    "    with open(path,'r') as dest_f:\n",
    "        data_iter = csv.reader(dest_f, delimiter = ',', quotechar = '\"')\n",
    "        data = [data for data in data_iter]\n",
    "    data_array = np.asarray(data, dtype = None)\n",
    "    return data_array\n",
    "\n",
    "data = readCSVasNumpy(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset - Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[1:,1].astype(int)\n",
    "print(labels)\n",
    "print(\"Positive class: \", sum(labels)/labels.size)\n",
    "print(\"Negative class: \", 1-sum(labels)/labels.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFields = [2, 4, 5, 6, 7, 9]\n",
    "features = data[1:, importantFields]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trivial dummy coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[:,1] = (features[:,1]==\"male\").astype(float)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIndexes = np.sort(np.random.choice(features.shape[0], int(features.shape[0]*0.7), replace=False))\n",
    "\n",
    "trainFeatures = features[trainIndexes]\n",
    "testFeatures = np.delete(features, trainIndexes, axis=0)\n",
    "\n",
    "trainLabels = labels[trainIndexes]\n",
    "testLabels = np.delete(labels, trainIndexes, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with missing data\n",
    "\n",
    "print(np.sum(trainFeatures == \"\", 0))\n",
    "print(np.sum(testFeatures == \"\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average of existing values on the TRAINING SET and \n",
    "# use it to substitute missing values in both sets\n",
    "\n",
    "agePresentMask = np.where(trainFeatures[:,2] != \"\")\n",
    "averageAge = np.mean(trainFeatures[agePresentMask,2].astype(float))\n",
    "trainFeatures[np.where(trainFeatures[:,2] == \"\"),2] = str(averageAge)\n",
    "\n",
    "agePresentMask = np.where(testFeatures[:,2] != \"\")\n",
    "testFeatures[np.where(testFeatures[:,2] == \"\"),2] = str(averageAge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert everything to floats so we can transform features\n",
    "\n",
    "trainFeatures = trainFeatures.astype(float)\n",
    "testFeatures = testFeatures.astype(float)\n",
    "print(trainFeatures)\n",
    "print(testFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features to interval [0,1]\n",
    "\n",
    "maxFeatures = np.max(trainFeatures, axis=0)\n",
    "minFeatures = np.min(trainFeatures, axis=0)\n",
    "\n",
    "trainFeatures = (trainFeatures - minFeatures) / (maxFeatures - minFeatures)\n",
    "testFeatures = (testFeatures - minFeatures) / (maxFeatures - minFeatures)\n",
    "\n",
    "print(trainFeatures)\n",
    "print(testFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "\\begin{equation*}\n",
    "f_w(x) = \\frac {1}{1+e^{-\\sum_{i=0}^n{w_i x_i}}}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRmodel:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.w = weights\n",
    "        self.b = bias\n",
    "        \n",
    "    def __init__(self, numFeatures):\n",
    "        self.w = np.ones(numFeatures)\n",
    "        self.b = np.ones(1)\n",
    "            \n",
    "    def evaluate(self, features):   \n",
    "        return sigmoid(np.dot(features, self.w) + self.b)\n",
    "    \n",
    "    def getModelParams(self):\n",
    "        return np.append(self.b, self.w)\n",
    "    \n",
    "    def setModelParams(self, params):\n",
    "        self.b = params[0]\n",
    "        self.w = params[1:]\n",
    "\n",
    "\n",
    "model = LRmodel(trainFeatures.shape[1])\n",
    "\n",
    "print(\"Model weights: \", model.w)\n",
    "print(\"Expected values: [1. 1. 1. 1. 1. 1.]\")\n",
    "\n",
    "print(\"Feature vector shape: \", trainFeatures[:,1:].shape)\n",
    "print(\"Expected values: (623, 6)\")\n",
    "\n",
    "print(\"First 3 model evaluations: \", model.evaluate(trainFeatures)[0:3])\n",
    "print(\"Expected values: [0.96802565 0.84066383 0.96923841]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "### Loss function:\n",
    "\n",
    "\\begin{equation*}\n",
    "E(w) = \\frac {1} {N} \\sum_{i=1}^N{L(f_w(x_i),y_i)} \n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "E(w) = \\frac {1} {N} \\sum_{i=1}^N{[-y_i\\log(f_w(x_i)) - (1-y_i)\\log(1 -f_w(x_i))]} \n",
    "\\end{equation*}\n",
    "\n",
    "### Gradient descent:\n",
    "\n",
    "\\begin{equation*}\n",
    "w_0 = w_0 - \\mu \\frac{1}{N}\\sum_{i=1}^N {(f_w(x_i) - y_i)}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "w_j = w_j - \\mu \\frac{1}{N}\\sum_{i=1}^N {(f_w(x_i) - y_i) x_{ij}}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def calculateLoss(self, features, labels):\n",
    "        # [TODO] Implement loss function based on the formulas above\n",
    "        f = self.model.evaluate(features)\n",
    "        error = -np.dot(labels, np.log(f)) - np.dot(1-labels,np.log(1-f))\n",
    "        return error / features.shape[0]\n",
    "    \n",
    "    def calculateGradients(self, features, labels):\n",
    "        # [TODO] Implement gradients function based on the formulas above\n",
    "        f = self.model.evaluate(features)\n",
    "        grad = np.matmul(np.append(np.ones((features.shape[0],1)), features, 1).T, (f - labels))\n",
    "        return grad / features.shape[0]\n",
    "    \n",
    "    def updateModel(self, gradient, learningRate):\n",
    "        # [TODO] Implement model update based on the gradients\n",
    "        w = self.model.getModelParams()\n",
    "        w -= learningRate * gradient\n",
    "        self.model.setModelParams(w)\n",
    "    \n",
    "    def train(self, features, labels, learningRate, iters, lossValues):\n",
    "        for i in iters:    \n",
    "            # [TODO] Implement one itteration of training using above functions\n",
    "            grad = self.calculateGradients(features, labels)\n",
    "            self.updateModel(grad, learningRate)\n",
    "    \n",
    "            loss = self.calculateLoss(features, labels)    \n",
    "            lossValues.append(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LRmodel(trainFeatures.shape[1])\n",
    "trainer = Trainer(model)\n",
    "\n",
    "\n",
    "print(\"Starting loss training: \", trainer.calculateLoss(trainFeatures, trainLabels))\n",
    "\n",
    "learningRate = 50\n",
    "lossValues = []\n",
    "iters = np.arange(1, 2000, 1)\n",
    "\n",
    "trainer.train(trainFeatures, trainLabels, learningRate, iters, lossValues)\n",
    "\n",
    "lossValues = np.array(lossValues)\n",
    "print(\"End loss training: \", lossValues[-1])\n",
    "\n",
    "plt.figure(1, figsize=(20, 8))\n",
    "plt.plot(iters, lossValues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def evaluate(self, features):\n",
    "        predictions = self.model.evaluate(features)\n",
    "        return predictions\n",
    "    \n",
    "    def calculateAPR(self, features, labels, threshold):\n",
    "        predictions = self.evaluate(features)\n",
    "        \n",
    "        numExamples = predictions.shape[0]\n",
    "        binaryPredictions = (predictions > threshold).astype(int)\n",
    "        \n",
    "        positivePredictions = np.where(binaryPredictions == 1)\n",
    "        negativePredictions = np.where(binaryPredictions == 0)\n",
    "        \n",
    "        # [TODO] Implement calculation of TP, FP, TN, FN, Precision, Recall and Accuracy\n",
    "        \n",
    "        # TP - Count of examples that were correctlly predicted as positive examples\n",
    "        TP = np.sum((labels[positivePredictions] == 1).astype(int))\n",
    "        \n",
    "        # FP - Count of examples that were incorectlly predicted as positive examples\n",
    "        FP = np.sum((labels[positivePredictions] == 0).astype(int))\n",
    "        \n",
    "        # TP - Count of examples that were incorectlly predicted as negative examples\n",
    "        FN = np.sum((labels[negativePredictions] == 1).astype(int))\n",
    "        \n",
    "        # TP - Count of examples that were correctlly predicted as negative examples\n",
    "        TN = np.sum((labels[negativePredictions] == 0).astype(int))\n",
    "        \n",
    "        if TP + FP > 0:\n",
    "            Precision = TP / (TP + FP)\n",
    "        else:\n",
    "            Precision = 1\n",
    "            \n",
    "        if TP + FN > 0:\n",
    "            Recall = TP / (TP + FN)\n",
    "        else:\n",
    "            Recall = 0\n",
    "            \n",
    "        Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        \n",
    "        return Precision, Recall, Accuracy\n",
    "    \n",
    "    def plotAPR(self, resultsTest, resultsTrain, ranges):\n",
    "        plt.figure(1, figsize=(20, 15))\n",
    "        plt.subplot(211)\n",
    "        plt.plot(ranges, np.matrix(resultsTrain)[:,0], ranges, np.matrix(resultsTrain)[:,1], ranges, np.matrix(resultsTrain)[:,2])\n",
    "        plt.subplot(212)\n",
    "        plt.plot(ranges, np.matrix(resultsTest)[:,0], ranges, np.matrix(resultsTest)[:,1], ranges, np.matrix(resultsTest)[:,2])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(model)\n",
    "t = np.arange(0., 1., 0.001)\n",
    "resultsTest = []\n",
    "resultsTrain = []\n",
    "for i in t:\n",
    "    resultsTest.append(evaluator.calculateAPR(testFeatures, testLabels, i))\n",
    "    resultsTrain.append(evaluator.calculateAPR(trainFeatures, trainLabels, i))\n",
    "\n",
    "evaluator.plotAPR(resultsTest, resultsTrain, t)\n",
    "\n",
    "print(\"Model w: \", model.w, \"\\nModel b: \", model.b)\n",
    "print(\"Accuracy: \", np.sum((model.evaluate(testFeatures)>0.5) == (testLabels==1))/testFeatures.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "### Regularized loss \n",
    "\\begin{equation*}\n",
    "E(w) = \\frac {1} {N} \\sum_{i=1}^N{L(h(x_i),y_i)} + \\frac {\\lambda}{2}\\sum_{i=1}^n{w_i^2}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "E(w) = \\frac {1} {N} \\sum_{i=1}^N{[-y_i \\log(f_w(x_i)) - (1-y_i) \\log(1 - f_w(x_i))]} + \\frac {\\lambda}{2}\\sum_{i=1}^n{w_i^2}\n",
    "\\end{equation*}\n",
    "\n",
    "### Gradient of regularized loss: \n",
    "\n",
    "\\begin{equation*}\n",
    "w_0 = w_0 - \\mu \\left[\\frac{1}{N} \\sum_{i=1}^N (f_w(x_i) - y_i)\\right]\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "w_j = w_j - \\mu \\left[\\frac{1}{N}\\sum_{i=1}^N (f_w(x_i) - y_i) x_{ij} + \\lambda w_j\\right]\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerReg:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def calculateLoss(self, features, labels, regParam):\n",
    "        # [TODO] Implement loss function based on the formulas above\n",
    "        f = self.model.evaluate(features)\n",
    "        loss = -np.dot(labels, np.log(f)) - np.dot(1-labels,np.log(1-f))\n",
    "        return loss / features.shape[0] + regParam * np.dot(self.model.w, self.model.w)/2\n",
    "    \n",
    "    def calculateGradients(self, features, labels, regParam):\n",
    "        # [TODO] Implement gradients function based on the formulas above\n",
    "        f = self.model.evaluate(features)\n",
    "        grad = np.matmul(np.append(np.ones((features.shape[0],1)), features, 1).T, (f - labels))\n",
    "        w = np.append([0], self.model.w)\n",
    "        return grad / features.shape[0] + regParam * w\n",
    "    \n",
    "    def updateModel(self, gradient, learningRate):\n",
    "        # [TODO] Implement model update based on the gradients\n",
    "        w = self.model.getModelParams()\n",
    "        w -= learningRate * gradient\n",
    "        self.model.setModelParams(w)\n",
    "        \n",
    "    def train(self, features, labels, regParam, learningRate, iters, lossValues):\n",
    "        for i in iters:    \n",
    "            # [TODO] Implement one itteration of training using above functions\n",
    "            grad = self.calculateGradients(features, labels, regParam)\n",
    "            self.updateModel(grad, learningRate)\n",
    "    \n",
    "            loss = self.calculateLoss(features, labels, regParam)\n",
    "            lossValues.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LRmodel(trainFeatures.shape[1])\n",
    "trainer = TrainerReg(model)\n",
    "\n",
    "regParam = 0.0001\n",
    "learningRate = 50\n",
    "\n",
    "print(\"Starting loss training: \", trainer.calculateLoss(trainFeatures, trainLabels, regParam))\n",
    "\n",
    "lossValues = []\n",
    "iters = np.arange(1, 2000, 1)\n",
    "\n",
    "trainer.train(trainFeatures, trainLabels, regParam, learningRate, iters, lossValues)\n",
    "\n",
    "lossValues = np.array(lossValues)\n",
    "print(\"End loss training: \", lossValues[-1])\n",
    "\n",
    "plt.figure(1, figsize=(20, 8))\n",
    "plt.plot(iters, lossValues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator(model)\n",
    "t = np.arange(0., 1., 0.001)\n",
    "resultsTest = []\n",
    "resultsTrain = []\n",
    "for i in t:\n",
    "    resultsTest.append(evaluator.calculateAPR(testFeatures, testLabels, i))\n",
    "    resultsTrain.append(evaluator.calculateAPR(trainFeatures, trainLabels, i))\n",
    "\n",
    "evaluator.plotAPR(resultsTest, resultsTrain, t)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "print(\"Model w: \", model.w, \"\\nModel b: \", model.b)\n",
    "print(\"Accuracy: \", np.sum((model.evaluate(testFeatures)>0.5) == (testLabels==1))/testFeatures.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIndexes = np.sort(np.random.choice(trainFeatures.shape[0], int(trainFeatures.shape[0]*0.7), replace=False))\n",
    "\n",
    "validFeatures = np.delete(trainFeatures, trainIndexes, axis=0)\n",
    "trainTrainFeatures = trainFeatures[trainIndexes]\n",
    "\n",
    "validLabels = np.delete(trainLabels, trainIndexes, axis=0)\n",
    "trainTrainLabels = trainLabels[trainIndexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [10**i for i in range(-7, 2)]\n",
    "lambdas.insert(0, 0)\n",
    "lambdas = np.array(lambdas)\n",
    "print(lambdas)\n",
    "\n",
    "learningRate = 0.1\n",
    "losses = []\n",
    "# TODO compute losses for different values of lambda and choose the best value lbest\n",
    "for l in lambdas:\n",
    "    model = LRmodel(trainTrainFeatures.shape[1])\n",
    "    trainer = TrainerReg(model)\n",
    "    lossValues = []\n",
    "    trainer.train(trainTrainFeatures, trainTrainLabels, l, learningRate, iters, lossValues)\n",
    "    loss = trainer.calculateLoss(validFeatures, validLabels, regParam)\n",
    "    losses.append(loss)\n",
    "\n",
    "lbest = lambdas[losses == min(losses)]\n",
    "lbest = lbest[0]\n",
    "\n",
    "model = LRmodel(trainFeatures.shape[1])\n",
    "trainer = TrainerReg(model)\n",
    "lossValues = []\n",
    "trainer.train(trainFeatures, trainLabels, lbest, learningRate, iters, lossValues)\n",
    "\n",
    "evaluator = Evaluator(model)\n",
    "t = np.arange(0., 1., 0.001)\n",
    "resultsTest = []\n",
    "resultsTrain = []\n",
    "for i in t:\n",
    "    resultsTest.append(evaluator.calculateAPR(testFeatures, testLabels, i))\n",
    "    resultsTrain.append(evaluator.calculateAPR(trainFeatures, trainLabels, i))\n",
    "\n",
    "evaluator.plotAPR(resultsTest, resultsTrain, t)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "print(\"Model w: \", model.w, \"\\nModel b: \", model.b)\n",
    "print(\"Selected lambda: \", lbest)\n",
    "print(\"Accuracy: \", np.sum((model.evaluate(testFeatures)>0.5) == (testLabels==1))/testFeatures.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
